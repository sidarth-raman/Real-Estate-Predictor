{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "78e19fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4bb11327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107899\n",
      "10582\n",
      "10224\n",
      "3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/bmvt9pvd0vd5t3yfr1yhfh580000gn/T/ipykernel_87136/713889739.py:2: DtypeWarning: Columns (13,20,53,54,59,60,61,62,63,64,65,66,67,68,69,70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  new_housing_csv = pd.read_csv('../data/new_housing.csv')\n"
     ]
    }
   ],
   "source": [
    "# housing_csv = pd.read_csv('../data/housing.csv')\n",
    "new_housing_csv = pd.read_csv('../data/new_housing.csv')\n",
    "\n",
    "#Length\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "#Mask Sale Price\n",
    "mask = (new_housing_csv[\"SALE PRICE\"]) > 100000.0\n",
    "new_housing_csv = new_housing_csv[mask]\n",
    "\n",
    "\n",
    "#Mask for Sold after 2010\n",
    "new_housing_csv['SALE DATE'] = pd.to_datetime(new_housing_csv['SALE DATE'])\n",
    "mask = (new_housing_csv[\"SALE DATE\"]).dt.year > 2011 \n",
    "new_housing_csv = new_housing_csv[mask]\n",
    "mask = (new_housing_csv[\"SALE DATE\"]).dt.year < 2015\n",
    "new_housing_csv = new_housing_csv[mask]\n",
    "\n",
    "#Length\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "#Only important rows\n",
    "new_housing_csv = new_housing_csv[[\"LAND VAL\", \"PARCEL VAL\", \"IMPROVE VAL\", \"SALE PRICE\", \"SQFT\", \"ROOMS\", \"BEDROOM\", \"BATH\", \"LIVING AREA\", \"GROSS AREA\", \"YEAR\", \"PERCENT GOOD\", \"STORIES\"]]\n",
    "\n",
    "\n",
    "#Remove Nulls\n",
    "mask = new_housing_csv.notnull().all(axis=1)\n",
    "new_housing_csv = new_housing_csv.loc[mask, :]\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "#Remove 0s\n",
    "mask = (new_housing_csv != 0).all(axis=1)\n",
    "new_housing_csv = new_housing_csv.loc[mask, :]\n",
    "\n",
    "\n",
    "#Length\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "\n",
    "housing_data = new_housing_csv[[\"LAND VAL\", \"PARCEL VAL\", \"SQFT\", \"ROOMS\", \"BEDROOM\", \"BATH\", \"LIVING AREA\", \"GROSS AREA\", \"YEAR\",\"PERCENT GOOD\", \"STORIES\"]]\n",
    "housing_label = new_housing_csv[[\"SALE PRICE\"]]\n",
    "\n",
    "\n",
    "\n",
    "# print(new_housing_csv)\n",
    "\n",
    "# housing_data = housing_csv[[\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"median_income\", \"population\", \"households\"]]\n",
    "# housing_label = housing_csv[[\"median_house_value\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4b3751dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_data\n",
    "Y = housing_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d2fc0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X, axis=0)\n",
    "sigma = np.std(X, axis=0)\n",
    "X = (X - mu) / sigma\n",
    "\n",
    "mu = np.mean(Y, axis=0)\n",
    "sigma = np.std(Y, axis=0)\n",
    "Y = (Y - mu) / sigma\n",
    "\n",
    "X = pd.DataFrame.to_numpy(X)\n",
    "Y = pd.DataFrame.to_numpy(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7edc0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, activation='relu', input_shape=[1])\n",
    "        self.dense2 = tf.keras.layers.Dense(units=64, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(units=1, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1a6efb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "37f3bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3601\n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "\n",
    "print(len(x))\n",
    "x_train = x[:3000]\n",
    "y_train = y[:3000]\n",
    "x_test = x[3001:3601]\n",
    "y_test = y[3001:3601]\n",
    "# Remove all instances where data is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.8882\n",
      "Epoch [200/10000], Loss: 0.8018\n",
      "Epoch [300/10000], Loss: 0.7522\n",
      "Epoch [400/10000], Loss: 0.7273\n",
      "Epoch [500/10000], Loss: 0.7127\n",
      "Epoch [600/10000], Loss: 0.6967\n",
      "Epoch [700/10000], Loss: 0.6883\n",
      "Epoch [800/10000], Loss: 0.6832\n",
      "Epoch [900/10000], Loss: 0.6794\n",
      "Epoch [1000/10000], Loss: 0.6763\n",
      "Epoch [1100/10000], Loss: 0.6737\n",
      "Epoch [1200/10000], Loss: 0.6716\n",
      "Epoch [1300/10000], Loss: 0.6696\n",
      "Epoch [1400/10000], Loss: 0.6679\n",
      "Epoch [1500/10000], Loss: 0.6663\n",
      "Epoch [1600/10000], Loss: 0.6648\n",
      "Epoch [1700/10000], Loss: 0.6631\n",
      "Epoch [1800/10000], Loss: 0.6524\n",
      "Epoch [1900/10000], Loss: 0.6382\n",
      "Epoch [2000/10000], Loss: 0.6331\n",
      "Epoch [2100/10000], Loss: 0.6307\n",
      "Epoch [2200/10000], Loss: 0.6291\n",
      "Epoch [2300/10000], Loss: 0.6277\n",
      "Epoch [2400/10000], Loss: 0.6264\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_train)\n",
    "        loss = loss_fn(y_train, y_pred)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ada14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b15f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true, sigma, mu, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the predictions given the true labels, based on a threshold value.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred (np.ndarray): An array of predicted labels.\n",
    "    y_true (np.ndarray): An array of true labels.\n",
    "    threshold (float): The threshold value to use for measuring accuracy.\n",
    "    \n",
    "    Returns:\n",
    "    float: The accuracy of the predictions as a percentage.\n",
    "    \"\"\"\n",
    "    # Compute the absolute difference between predicted and true values\n",
    "    \n",
    "    \n",
    "    y_pred = (y_pred * sigma) + mu\n",
    "    y_true = (y_true * sigma) + mu\n",
    "\n",
    "    diff = np.abs(y_pred - y_true)\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc = np.mean(diff <= threshold * y_pred)\n",
    "    \n",
    "    # Convert to percentage\n",
    "    acc_pct = acc * 100\n",
    "    \n",
    "    return acc_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fe4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_test)\n",
    "loss = loss_fn(y_test, pred)\n",
    "accuracy(pred, y_test, sigma, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93887275",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = model.layers[0].get_weights()\n",
    "print(weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
