{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78e19fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bb11327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107899\n",
      "17728\n",
      "17087\n",
      "6215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/bmvt9pvd0vd5t3yfr1yhfh580000gn/T/ipykernel_87136/2556968537.py:2: DtypeWarning: Columns (13,20,53,54,59,60,61,62,63,64,65,66,67,68,69,70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  new_housing_csv = pd.read_csv('../data/new_housing.csv')\n"
     ]
    }
   ],
   "source": [
    "# housing_csv = pd.read_csv('../data/housing.csv')\n",
    "new_housing_csv = pd.read_csv('../data/new_housing.csv')\n",
    "\n",
    "#Length\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "#Mask Sale Price\n",
    "mask = (new_housing_csv[\"SALE PRICE\"]) > 100000.0\n",
    "new_housing_csv = new_housing_csv[mask]\n",
    "\n",
    "\n",
    "#Mask for Sold after 2010\n",
    "new_housing_csv['SALE DATE'] = pd.to_datetime(new_housing_csv['SALE DATE'])\n",
    "mask = (new_housing_csv[\"SALE DATE\"]).dt.year > 2010\n",
    "new_housing_csv = new_housing_csv[mask]\n",
    "\n",
    "#Length\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "#Only important rows\n",
    "new_housing_csv = new_housing_csv[[\"LAND VAL\", \"PARCEL VAL\", \"IMPROVE VAL\", \"SALE PRICE\", \"SQFT\", \"ROOMS\", \"BEDROOM\", \"BATH\", \"LIVING AREA\", \"GROSS AREA\", \"YEAR\", \"PERCENT GOOD\", \"STORIES\"]]\n",
    "\n",
    "\n",
    "#Remove Nulls\n",
    "mask = new_housing_csv.notnull().all(axis=1)\n",
    "new_housing_csv = new_housing_csv.loc[mask, :]\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "#Remove 0s\n",
    "mask = (new_housing_csv != 0).all(axis=1)\n",
    "new_housing_csv = new_housing_csv.loc[mask, :]\n",
    "\n",
    "\n",
    "#Length\n",
    "print(len(new_housing_csv))\n",
    "\n",
    "\n",
    "housing_data = new_housing_csv[[\"LAND VAL\", \"PARCEL VAL\", \"SQFT\", \"ROOMS\", \"BEDROOM\", \"BATH\", \"LIVING AREA\", \"GROSS AREA\", \"YEAR\",\"PERCENT GOOD\", \"STORIES\"]]\n",
    "housing_label = new_housing_csv[[\"SALE PRICE\"]]\n",
    "\n",
    "\n",
    "\n",
    "# print(new_housing_csv)\n",
    "\n",
    "# housing_data = housing_csv[[\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"median_income\", \"population\", \"households\"]]\n",
    "# housing_label = housing_csv[[\"median_house_value\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b3751dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_data\n",
    "Y = housing_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2fc0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X, axis=0)\n",
    "sigma = np.std(X, axis=0)\n",
    "X = (X - mu) / sigma\n",
    "\n",
    "mu = np.mean(Y, axis=0)\n",
    "sigma = np.std(Y, axis=0)\n",
    "Y = (Y - mu) / sigma\n",
    "\n",
    "X = pd.DataFrame.to_numpy(X)\n",
    "Y = pd.DataFrame.to_numpy(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7edc0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, activation='relu', input_shape=[5])\n",
    "        self.dense2 = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(units=1, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a6efb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "37f3bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6215\n"
     ]
    }
   ],
   "source": [
    "x = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "\n",
    "print(len(x))\n",
    "x_train = x[:5000]\n",
    "y_train = y[:5000]\n",
    "x_test = x[5001:6215]\n",
    "y_test = y[5001:6215]\n",
    "# Remove all instances where data is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0493b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.4001\n",
      "Epoch [200/10000], Loss: 0.3781\n",
      "Epoch [300/10000], Loss: 0.3629\n",
      "Epoch [400/10000], Loss: 0.3501\n",
      "Epoch [500/10000], Loss: 0.3389\n",
      "Epoch [600/10000], Loss: 0.3281\n",
      "Epoch [700/10000], Loss: 0.3185\n",
      "Epoch [800/10000], Loss: 0.3091\n",
      "Epoch [900/10000], Loss: 0.3000\n",
      "Epoch [1000/10000], Loss: 0.2910\n",
      "Epoch [1100/10000], Loss: 0.2841\n",
      "Epoch [1200/10000], Loss: 0.2860\n",
      "Epoch [1300/10000], Loss: 0.2827\n",
      "Epoch [1400/10000], Loss: 0.2851\n",
      "Epoch [1500/10000], Loss: 0.2719\n",
      "Epoch [1600/10000], Loss: 0.2731\n",
      "Epoch [1700/10000], Loss: 0.2675\n",
      "Epoch [1800/10000], Loss: 0.2627\n",
      "Epoch [1900/10000], Loss: 0.2525\n",
      "Epoch [2000/10000], Loss: 0.2580\n",
      "Epoch [2100/10000], Loss: 0.2532\n",
      "Epoch [2200/10000], Loss: 0.2488\n",
      "Epoch [2300/10000], Loss: 0.2500\n",
      "Epoch [2400/10000], Loss: 0.2326\n",
      "Epoch [2500/10000], Loss: 0.2365\n",
      "Epoch [2600/10000], Loss: 0.2384\n",
      "Epoch [2700/10000], Loss: 0.2281\n",
      "Epoch [2800/10000], Loss: 0.2243\n",
      "Epoch [2900/10000], Loss: 0.2282\n",
      "Epoch [3000/10000], Loss: 0.2300\n",
      "Epoch [3100/10000], Loss: 0.2180\n",
      "Epoch [3200/10000], Loss: 0.2169\n",
      "Epoch [3300/10000], Loss: 0.2179\n",
      "Epoch [3400/10000], Loss: 0.2110\n",
      "Epoch [3500/10000], Loss: 0.2039\n",
      "Epoch [3600/10000], Loss: 0.2158\n",
      "Epoch [3700/10000], Loss: 0.2003\n",
      "Epoch [3800/10000], Loss: 0.2136\n",
      "Epoch [3900/10000], Loss: 0.1956\n",
      "Epoch [4000/10000], Loss: 0.2097\n",
      "Epoch [4100/10000], Loss: 0.1949\n",
      "Epoch [4200/10000], Loss: 0.2037\n",
      "Epoch [4300/10000], Loss: 0.1898\n",
      "Epoch [4400/10000], Loss: 0.1991\n",
      "Epoch [4500/10000], Loss: 0.1877\n",
      "Epoch [4600/10000], Loss: 0.1964\n",
      "Epoch [4700/10000], Loss: 0.1877\n",
      "Epoch [4800/10000], Loss: 0.1831\n",
      "Epoch [4900/10000], Loss: 0.1801\n",
      "Epoch [5000/10000], Loss: 0.1832\n",
      "Epoch [5100/10000], Loss: 0.1795\n",
      "Epoch [5200/10000], Loss: 0.1844\n",
      "Epoch [5300/10000], Loss: 0.1750\n",
      "Epoch [5400/10000], Loss: 0.1819\n",
      "Epoch [5500/10000], Loss: 0.1735\n",
      "Epoch [5600/10000], Loss: 0.1792\n",
      "Epoch [5700/10000], Loss: 0.1682\n",
      "Epoch [5800/10000], Loss: 0.1823\n",
      "Epoch [5900/10000], Loss: 0.1661\n",
      "Epoch [6000/10000], Loss: 0.1754\n",
      "Epoch [6100/10000], Loss: 0.1654\n",
      "Epoch [6200/10000], Loss: 0.1752\n",
      "Epoch [6300/10000], Loss: 0.1601\n",
      "Epoch [6400/10000], Loss: 0.1740\n",
      "Epoch [6500/10000], Loss: 0.1582\n",
      "Epoch [6600/10000], Loss: 0.1659\n",
      "Epoch [6700/10000], Loss: 0.1585\n",
      "Epoch [6800/10000], Loss: 0.1623\n",
      "Epoch [6900/10000], Loss: 0.1620\n",
      "Epoch [7000/10000], Loss: 0.1509\n",
      "Epoch [7100/10000], Loss: 0.1549\n",
      "Epoch [7200/10000], Loss: 0.1653\n",
      "Epoch [7300/10000], Loss: 0.1550\n",
      "Epoch [7400/10000], Loss: 0.1466\n",
      "Epoch [7500/10000], Loss: 0.1474\n",
      "Epoch [7600/10000], Loss: 0.1516\n",
      "Epoch [7700/10000], Loss: 0.1587\n",
      "Epoch [7800/10000], Loss: 0.1529\n",
      "Epoch [7900/10000], Loss: 0.1436\n",
      "Epoch [8000/10000], Loss: 0.1494\n",
      "Epoch [8100/10000], Loss: 0.1593\n",
      "Epoch [8200/10000], Loss: 0.1490\n",
      "Epoch [8300/10000], Loss: 0.1392\n",
      "Epoch [8400/10000], Loss: 0.1430\n",
      "Epoch [8500/10000], Loss: 0.1517\n",
      "Epoch [8600/10000], Loss: 0.1390\n",
      "Epoch [8700/10000], Loss: 0.1392\n",
      "Epoch [8800/10000], Loss: 0.1481\n",
      "Epoch [8900/10000], Loss: 0.1481\n",
      "Epoch [9000/10000], Loss: 0.1379\n",
      "Epoch [9100/10000], Loss: 0.1322\n",
      "Epoch [9200/10000], Loss: 0.1356\n",
      "Epoch [9300/10000], Loss: 0.1459\n",
      "Epoch [9400/10000], Loss: 0.1411\n",
      "Epoch [9500/10000], Loss: 0.1305\n",
      "Epoch [9600/10000], Loss: 0.1301\n",
      "Epoch [9700/10000], Loss: 0.1362\n",
      "Epoch [9800/10000], Loss: 0.1427\n",
      "Epoch [9900/10000], Loss: 0.1300\n",
      "Epoch [10000/10000], Loss: 0.1270\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_train)\n",
    "        loss = loss_fn(y_train, y_pred)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d4ada14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'linear_regression_9/dense_27/kernel:0' shape=(11, 128) dtype=float32, numpy=\n",
      "array([[-0.3291976 ,  0.18513829,  0.07537496, ..., -0.02731233,\n",
      "        -0.09792882, -0.04650657],\n",
      "       [ 0.08806617, -0.26378837,  0.07357538, ..., -0.26298946,\n",
      "         0.06661487, -0.10401405],\n",
      "       [-0.01087888, -0.17683797, -0.10825297, ..., -0.00503848,\n",
      "        -0.00734863, -0.19398376],\n",
      "       ...,\n",
      "       [ 0.02465856,  0.35552397,  0.01854089, ...,  0.06497732,\n",
      "         0.00971519, -0.0451232 ],\n",
      "       [-0.23539421, -0.08004604,  0.24673244, ...,  0.12194559,\n",
      "         0.42519462, -0.1441144 ],\n",
      "       [-0.27245414,  0.16521119, -0.15240367, ..., -0.15928137,\n",
      "        -0.20171343,  0.15645276]], dtype=float32)>, <tf.Variable 'linear_regression_9/dense_27/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([ 1.29235998e-01,  3.41025777e-02, -1.51369110e-01, -1.84721500e-01,\n",
      "       -6.67115580e-03, -6.67378902e-02, -1.15815669e-01, -3.56149003e-02,\n",
      "        2.54148226e-02, -9.73720998e-02, -2.46300511e-02, -9.23112500e-03,\n",
      "       -1.33461267e-01, -1.63556278e-01, -6.48936033e-02, -7.27733970e-02,\n",
      "       -3.74724716e-02, -1.29799798e-01, -1.09261632e-01,  9.52014420e-03,\n",
      "       -1.09814100e-01, -7.77472630e-02, -2.82794666e-02, -2.34313458e-02,\n",
      "       -8.90310556e-02, -2.83578932e-01, -8.35458189e-02, -2.28996888e-01,\n",
      "       -1.65223554e-01, -3.36847967e-03, -4.56291512e-02,  7.60630816e-02,\n",
      "        1.31639957e-01, -2.22572044e-01, -6.74437433e-02, -8.45081806e-02,\n",
      "        2.09762424e-01,  1.68525442e-01, -1.59752965e-02, -5.06821312e-02,\n",
      "        9.58443526e-03, -3.01113557e-02, -2.91375462e-02,  4.86953370e-02,\n",
      "       -1.20915182e-01,  1.16044864e-01,  8.81154463e-02, -1.54885188e-01,\n",
      "        5.48862778e-02,  9.80725791e-03, -1.44049123e-01, -1.44030422e-01,\n",
      "       -8.38164911e-02,  4.34688888e-02,  1.42725557e-01,  9.46402997e-02,\n",
      "       -1.13184050e-01,  9.75520760e-02,  1.00794910e-02, -1.55236557e-01,\n",
      "        1.04998454e-01,  9.20941681e-02, -1.51337937e-01, -1.64998785e-01,\n",
      "       -2.61134747e-03,  1.59064494e-02, -1.12280212e-01, -3.38947117e-01,\n",
      "        1.33812251e-02,  4.97714914e-02, -8.42509121e-02, -3.99131514e-02,\n",
      "        1.76138148e-01, -2.26520188e-02, -4.76221144e-02, -9.65631157e-02,\n",
      "       -3.58841680e-02, -8.26650634e-02, -7.40263462e-02, -8.72297958e-02,\n",
      "       -2.26275608e-01, -1.18139021e-01, -4.47415337e-02,  6.70469031e-02,\n",
      "        1.00509999e-02, -9.72867310e-02,  1.21001229e-02, -3.84660810e-02,\n",
      "       -1.56867892e-01, -2.03157049e-02,  1.55691579e-02,  7.68595487e-02,\n",
      "        9.90015827e-03, -3.91865671e-02, -5.04805781e-02,  1.08452868e-02,\n",
      "       -1.17967688e-01, -1.00462072e-01, -8.77869874e-02, -1.64352066e-03,\n",
      "       -8.20599496e-02,  5.38161585e-05, -2.04268377e-02,  9.71124973e-03,\n",
      "        6.74574673e-02,  1.82442158e-01, -1.71012864e-01,  5.29685691e-02,\n",
      "        1.68123133e-02, -2.27765497e-02, -7.96583816e-02, -5.54151647e-02,\n",
      "       -1.59897599e-02, -1.11187279e-01,  2.97431201e-02, -5.09662218e-02,\n",
      "       -1.41155884e-01,  2.33027879e-02, -1.13958888e-01, -1.12291783e-01,\n",
      "        1.56642571e-01, -1.69161335e-01, -1.41969204e-01, -1.82730816e-02,\n",
      "        2.29794979e-02,  9.25733149e-02,  2.44313348e-02, -6.63279667e-02],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "85b15f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true, sigma, mu, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the predictions given the true labels, based on a threshold value.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred (np.ndarray): An array of predicted labels.\n",
    "    y_true (np.ndarray): An array of true labels.\n",
    "    threshold (float): The threshold value to use for measuring accuracy.\n",
    "    \n",
    "    Returns:\n",
    "    float: The accuracy of the predictions as a percentage.\n",
    "    \"\"\"\n",
    "    # Compute the absolute difference between predicted and true values\n",
    "    \n",
    "    \n",
    "    y_pred = (y_pred * sigma) + mu\n",
    "    y_true = (y_true * sigma) + mu\n",
    "\n",
    "    diff = np.abs(y_pred - y_true)\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc = np.mean(diff <= threshold * y_pred)\n",
    "    \n",
    "    # Convert to percentage\n",
    "    acc_pct = acc * 100\n",
    "    \n",
    "    return acc_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b8fe4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 598055.44]\n",
      " [ 678932.1 ]\n",
      " [ 797673.25]\n",
      " ...\n",
      " [1290857.  ]\n",
      " [1200040.4 ]\n",
      " [1646885.  ]], shape=(1214, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 445000.]\n",
      " [ 669000.]\n",
      " [ 657000.]\n",
      " ...\n",
      " [ 810000.]\n",
      " [ 950000.]\n",
      " [1690000.]], shape=(1214, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58.97858319604613"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(x_test)\n",
    "loss = loss_fn(y_test, pred)\n",
    "accuracy(pred, y_test, sigma, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "93887275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3291976   0.18513829  0.07537496 ... -0.02731233 -0.09792882\n",
      "  -0.04650657]\n",
      " [ 0.08806617 -0.26378837  0.07357538 ... -0.26298946  0.06661487\n",
      "  -0.10401405]\n",
      " [-0.01087888 -0.17683797 -0.10825297 ... -0.00503848 -0.00734863\n",
      "  -0.19398376]\n",
      " ...\n",
      " [ 0.02465856  0.35552397  0.01854089 ...  0.06497732  0.00971519\n",
      "  -0.0451232 ]\n",
      " [-0.23539421 -0.08004604  0.24673244 ...  0.12194559  0.42519462\n",
      "  -0.1441144 ]\n",
      " [-0.27245414  0.16521119 -0.15240367 ... -0.15928137 -0.20171343\n",
      "   0.15645276]]\n"
     ]
    }
   ],
   "source": [
    "weights, bias = model.layers[0].get_weights()\n",
    "print(weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
